{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian retail store sales prediction\n",
    "\n",
    "This notebook shows the work of Udacity machine learning final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import psutil\n",
    "import numpy as np\n",
    "import itertools\n",
    "import gc\n",
    "import sys \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('./items.csv')\n",
    "cat = pd.read_csv('./item_categories.csv')\n",
    "shop = pd.read_csv('./shops.csv')\n",
    "sales = pd.read_csv('./sales_train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# showing part of the dataset\n",
    "from IPython.display import display\n",
    "for tab in [items,cat,shop,sales,test]:\n",
    "    display(tab.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the shape of the dataset\n",
    "print(items.shape)\n",
    "print(sales.shape)\n",
    "print(cat.shape)\n",
    "print(shop.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check memory usage\n",
    "def memcheck():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(process.memory_info().rss)\n",
    "# check new product launched in new period\n",
    "def newproduct(datebk):\n",
    "    return np.setdiff1d(np.unique(alldat.loc[alldat.date_block_num==datebk,'item_id']),\n",
    "                        np.unique(alldat.loc[alldat.date_block_num<datebk,'item_id']))\n",
    "# plot new graph\n",
    "def plot(ts, xl,yl, title=''):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.xlabel(xl)\n",
    "    plt.ylabel(yl)\n",
    "   \n",
    "    plt.title(title)\n",
    "    plt.plot(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(sales['item_cnt_day'], color = 'blue', edgecolor = 'black'\n",
    "        )\n",
    "plt.title('distribution of daily sales'\n",
    "         )\n",
    "plt.xlabel('daily sales')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = sum(sales['item_cnt_day']<0)\n",
    "zero = sum(sales['item_cnt_day']==0)\n",
    "one_percent = sum(sales['item_cnt_day']==1)/len(sales)\n",
    "print('the number of instance with negative sales is {} '.format(neg))\n",
    "print('the number of instance with zero sales is {} '.format(zero))\n",
    "print('the proportion of instance with sales =1  is {} '.format(one_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.DataFrame(sales['item_price'].describe())\n",
    "stat['item_price'] = np.round(stat['item_price'])\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = pd.DataFrame(sales['item_cnt_day'].describe())\n",
    "stat['item_cnt_day'] = np.round(stat['item_cnt_day'])\n",
    "stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refund_count_item = sales.loc[sales.item_cnt_day<-0].groupby(['item_id','date_block_num'])['item_cnt_day'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "salestrend = sales.groupby(['date_block_num'])['item_cnt_day'].sum()\n",
    "plot(salestrend,'date_block_num','sales','sales trend by month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,40))\n",
    "for i in range(0,60):\n",
    "    trend =  sales.loc[sales.shop_id==i].groupby(['date_block_num'])['item_cnt_day'].sum()\n",
    "    ax = fig.add_subplot(10,6,i+1)\n",
    "    ax.set_title('shop %s'%i)\n",
    "    ax.plot(trend)\n",
    "    ax.set_ylabel('monthly_sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By decomposing the total sales into sales for each of the store, it can be seen that most of the sales data show a similar pattern to the sales data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expansion of the sales dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_num =len(np.unique(items['item_id']))\n",
    "shop_num = len(np.unique(shop['shop_id']))\n",
    "print('total number of shop: {}'.format(shop_num))\n",
    "print('total number of item: {}'.format(item_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_block = sales['date_block_num'].unique()\n",
    "grid=[]\n",
    "for d in date_block:\n",
    "    #number of unique shops and items\n",
    "    allshops = sales.loc[sales.date_block_num==d,'shop_id'].unique()\n",
    "    allitems = sales.loc[sales.date_block_num==d, 'item_id'].unique()\n",
    "    \n",
    "    grid.append(list(itertools.product(*[allshops,allitems,[d]])))\n",
    "\n",
    "grids = np.vstack(grid)\n",
    "# combine the shop-item pairing with sales dataset  \n",
    "grids_df = pd.DataFrame(grids,columns=['shop_id','item_id','date_block_num'])\n",
    "sales_month = sales.groupby(['shop_id','item_id','date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\n",
    "sales_month.columns = ['shop_id','item_id','date_block_num','monthly_sales']\n",
    "\n",
    "train_data = pd.merge(grids_df, sales_month, on=['shop_id','item_id','date_block_num'],how='left' ).fillna(0)\n",
    "sales = pd.merge(sales, items[['item_id','item_category_id']], how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del grids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('the shape of expanded dataset is {}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data has a month period of 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = test['ID']\n",
    "test.drop('ID',axis=1, inplace=True)\n",
    "test['date_block_num']=np.nan\n",
    "test['date_block_num'] = test['date_block_num'].fillna(34)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = train_data.append(test)\n",
    "train_l = train_data.shape[0]\n",
    "test_l = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct variables(city, item category , year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop['city'] = shop.shop_name.str.split(expand=True)[0]\n",
    "\n",
    "alldat = pd.merge(alldat,shop, on='shop_id', how='left')\n",
    "sales = pd.merge(sales,shop, on='shop_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = alldat.drop('shop_name',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['date'] = pd.to_datetime(sales['date'], format='%d.%m.%Y')\n",
    "sales['month']= sales['date'].dt.month\n",
    "\n",
    "date_df = sales[['date','date_block_num','month']].copy()\n",
    "date_df = date_df.drop('date',axis=1)\n",
    "date_df = date_df.drop_duplicates(keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = pd.merge(alldat, date_df, how='left', on='date_block_num')\n",
    "alldat = pd.merge(alldat, items[['item_id','item_category_id']], how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat.loc[train_l:,'month'] = alldat.loc[train_l:,'month'].fillna(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat['year'] = alldat['date_block_num']//12\n",
    "sales['year'] = sales['date_block_num']//12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_month_item = sales.groupby(['item_id','date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\n",
    "sales_month_item.columns = ['item_id','date_block_num','monthly_sales_item']\n",
    "\n",
    "sales_month_shop = sales.groupby(['shop_id','date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\n",
    "sales_month_shop.columns = ['shop_id','date_block_num','monthly_sales_shop']\n",
    "'''\n",
    "sales_month_cat = sales.groupby(['item_category_id','date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\n",
    "sales_month_cat.columns = ['item_category_id','date_block_num','monthly_sales_cat']\n",
    "\n",
    "sales_month_city = sales.groupby(['city','date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\n",
    "sales_month_city.columns = ['city','date_block_num','monthly_sales_city']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = pd.merge(alldat, sales_month_item, how='left', on=['item_id','date_block_num'])\n",
    "alldat = pd.merge(alldat, sales_month_shop, how='left', on=['shop_id','date_block_num'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_stat_item = sales.groupby(['item_id'],as_index=False).agg({'item_price':['mean','std']}).fillna(0)\n",
    "price_stat_item.columns = ['item_id','item_price_mean','item_price_std']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_stat_shop = sales.groupby(['shop_id'],as_index=False).agg({'item_price':['mean','std']}).fillna(0)\n",
    "price_stat_shop.columns = ['shop_id','shop_price_mean','shop_price_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = pd.merge(alldat, price_stat_item,on=['item_id'], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = pd.merge(alldat, price_stat_shop ,on=['shop_id'], how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del price_stat_item\n",
    "del price_stat_shop\n",
    "del grids\n",
    "del grid\n",
    "del sales_month\n",
    "del sales_month_item\n",
    "del sales_month_shop\n",
    "\n",
    "del date_df\n",
    "del date_block\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat[['item_price_mean','item_price_std']] = alldat[['item_price_mean','item_price_std']].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "lb.fit(alldat['city'])\n",
    "\n",
    "alldat['city']=lb.fit_transform(alldat['city'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat.loc[:train_l-1,'monthly_sales'] = alldat.loc[:train_l-1,'monthly_sales'].clip(0,30)\n",
    "# clipping them between 0 and 90 percentile\n",
    "for f in ['monthly_sales_shop','monthly_sales_item']:#\n",
    "    alldat.loc[:train_l,f] = alldat.loc[:train_l,f].clip(0,alldat.loc[:train_l,f].quantile(0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  alldat.loc[:train_l-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## discretize target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bin_array = [0,1,2,3,5,30.1]\n",
    "# count the number of instance in each bin\n",
    "train_data['discretized_sales'] = np.digitize(train_data['monthly_sales'],bin_array)\n",
    "\n",
    "# generate discretized feature grouped by shop\n",
    "count_shop = train_data.groupby(['shop_id','discretized_sales','year']).monthly_sales.count()\n",
    "count_shop = count_shop.reset_index().pivot_table(index=['shop_id','year'],columns=['discretized_sales'],values='monthly_sales').fillna(0)\n",
    "\n",
    "count_shop.columns = ['shop_discrete_'+str(c) for c in count_shop.columns]\n",
    "\n",
    "\n",
    "alldat = pd.merge(alldat, count_shop, on=['shop_id','year'], how='left')\n",
    "\n",
    "\n",
    "\n",
    "alldat['shop_count'] = alldat['shop_discrete_1']+alldat['shop_discrete_5']+alldat['shop_discrete_2']+alldat['shop_discrete_3']+alldat['shop_discrete_4']\n",
    "\n",
    "for i in [1,2,3,4,5]:\n",
    "    alldat['shop_discrete_'+str(i)] = alldat['shop_discrete_'+str(i)] / alldat['shop_count']\n",
    "\n",
    "alldat.drop(['shop_count'],axis=1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by item category and year\n",
    "'''\n",
    "train_data['discretized_sales'] = np.digitize(train_data['monthly_sales'],bin_array)\n",
    "\n",
    "\n",
    "count_shop = train_data.groupby(['item_category_id','discretized_sales','year']).monthly_sales.count()\n",
    "count_shop = count_shop.reset_index().pivot_table(index=['item_category_id','year'],columns=['discretized_sales'],values='monthly_sales').fillna(0)\n",
    "count_shop.columns = ['cat_discrete_'+str(c) for c in count_shop.columns]\n",
    "\n",
    "\n",
    "alldat = pd.merge(alldat, count_shop, on=['item_category_id','year'], how='left')\n",
    "\n",
    "alldat['cat_count'] = alldat['cat_discrete_1']+alldat['cat_discrete_5']+alldat['cat_discrete_2']+alldat['cat_discrete_3']+alldat['cat_discrete_4']\n",
    "\n",
    "for i in [1,2,3,4,5]:\n",
    "    alldat['cat_discrete_'+str(i)] = alldat['cat_discrete_'+str(i)] / alldat['cat_count']\n",
    "    \n",
    "\n",
    "alldat.drop(['cat_count'],axis=1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by shop and month\n",
    "'''\n",
    "train_data['discretized_sales'] = np.digitize(train_data['monthly_sales'],bin_array)\n",
    "\n",
    "\n",
    "count_shop = train_data.groupby(['shop_id','discretized_sales','month']).monthly_sales.count()\n",
    "count_shop = count_shop.reset_index().pivot_table(index=['shop_id','month'],columns=['discretized_sales'],values='monthly_sales').fillna(0)\n",
    "count_shop.columns = ['shop_discrete_month_'+str(c) for c in count_shop.columns]\n",
    "\n",
    "\n",
    "alldat = pd.merge(alldat, count_shop, on=['shop_id','month'], how='left')\n",
    "\n",
    "\n",
    "alldat['shop_count'] = alldat['shop_discrete_month_1']+alldat['shop_discrete_month_5']+alldat['shop_discrete_month_2']+alldat['shop_discrete_month_3']+alldat['shop_discrete_month_4']\n",
    "\n",
    "for i in [1,2,3,4,5]:\n",
    "    alldat['shop_discrete_month_'+str(i)] = alldat['shop_discrete_month_'+str(i)] / alldat['shop_count']\n",
    "\n",
    "alldat.drop(['shop_count'],axis=1, inplace=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "def mean_encoding_bydate(df,feature):\n",
    "    name=feature\n",
    "    if(type(feature) ==list):\n",
    "        name='-'.join(feature)\n",
    "    \n",
    "    df[name+'_mean_encoded']=np.nan\n",
    "    for d in tqdm_notebook(df['date_block_num'].unique()):\n",
    "        \n",
    "        past_date_mask = (df['date_block_num']<d) \n",
    "        current_date_mask = (df['date_block_num']==d) \n",
    "        \n",
    "        \n",
    "        \n",
    "        mean = df.loc[past_date_mask].groupby(feature).monthly_sales.mean()\n",
    "        mean = mean.reset_index()\n",
    "        temp = pd.merge(df.loc[current_date_mask],mean, how='left', on=feature)\n",
    "        temp.set_index(df.loc[current_date_mask].index,inplace=True)\n",
    "        \n",
    "        df.loc[current_date_mask, name+'_mean_encoded'] = temp.monthly_sales_y\n",
    "        \n",
    "        max_ = df.loc[past_date_mask].groupby(feature).monthly_sales.max()\n",
    "        max_ = max_.reset_index()\n",
    "        temp2 = pd.merge(df.loc[current_date_mask],max_, how='left', on=feature)\n",
    "        temp2.set_index(df.loc[current_date_mask].index,inplace=True)\n",
    "        \n",
    "        df.loc[current_date_mask, name+'_max_encoded'] = temp2.monthly_sales_y\n",
    "        \n",
    "        df = df.fillna(0.3343)\n",
    "    \n",
    "        del mean\n",
    "        del max_\n",
    "        gc.collect()\n",
    "    \n",
    "        \n",
    "    #print(\"The coorelation between target and {} is {}\".format(name, np.corrcoef(df[name+'_mean_encoded'],df.monthly_sales)[0][1]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38821e7e531c455d861fb4ef381abbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f_arr = ['shop_id','item_id','city','item_category_id']\n",
    "for f in f_arr:\n",
    "    alldat = mean_encoding_bydate(alldat,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the global average of monthly sales is 0.3067073800034363\n"
     ]
    }
   ],
   "source": [
    "print('the global average of monthly sales is {}'.format(np.mean(alldat['monthly_sales'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat.loc[:,'shop_id_mean_encoded':] = alldat.loc[:,'shop_id_mean_encoded':].fillna(0.3343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a839cea0a94a0fa5a275275340d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 months lag features created\n",
      "2 months lag features created\n",
      "3 months lag features created\n",
      "6 months lag features created\n",
      "12 months lag features created\n",
      "\n",
      "CPU times: user 1min 8s, sys: 25.5 s, total: 1min 34s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "# increase date_block_num by the number of lags and merge it to the training set\n",
    "shift_range=[1,2,3,6,12]\n",
    "for lag in tqdm_notebook(shift_range):\n",
    "    feat_tolag = alldat[['date_block_num','item_id','shop_id','monthly_sales_item',\n",
    "       'monthly_sales_shop','item_price_mean','monthly_sales']].copy()\n",
    "     \n",
    "    # increase date_block_num by the number of lags and merge it to the training set\n",
    "    feat_tolag['date_block_num'] = feat_tolag['date_block_num']+lag\n",
    "    feat_tolag.columns =[c+'_lag_'+str(lag) if c.startswith('month') ==True or c.startswith('item_price') == True  else c for c in feat_tolag.columns]\n",
    "\n",
    "    alldat = pd.merge(alldat, feat_tolag, how='left', on=['date_block_num','item_id','shop_id'], suffixes=['','_y'])\n",
    "    del feat_tolag\n",
    "    print('{} months lag features created'.format(lag)) \n",
    "    gc.collect()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat.loc[:,'monthly_sales_item_lag_1':] = alldat.loc[:,'monthly_sales_item_lag_1':].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downcast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the numeric types to reduce memory usage\n",
    "int_feats = alldat.select_dtypes(include=[np.int64]).columns\n",
    "float_feats = alldat.select_dtypes(include=[np.float64]).columns\n",
    "\n",
    "alldat[int_feats] = alldat[int_feats].astype(np.int32)\n",
    "alldat[float_feats] = alldat[float_feats].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference between lag features\n",
    "for f in ['monthly_sales_item',\n",
    "       'monthly_sales_shop','item_price_mean','monthly_sales']:\n",
    "    for lag1, lag2 in zip(shift_range[0:3],shift_range[1:]):\n",
    "        alldat[f+'_diff_'+str(lag1)+str(lag2)] = alldat[f+'_lag_'+str(lag1)] -  alldat[f+'_lag_'+str(lag2)]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11128050 entries, 0 to 11128049\n",
      "Data columns (total 54 columns):\n",
      "date_block_num                   float32\n",
      "item_id                          int32\n",
      "monthly_sales                    float32\n",
      "shop_id                          int32\n",
      "city                             int32\n",
      "month                            float32\n",
      "item_category_id                 int32\n",
      "year                             float32\n",
      "monthly_sales_item               float32\n",
      "monthly_sales_shop               float32\n",
      "item_price_mean                  float32\n",
      "item_price_std                   float32\n",
      "shop_price_mean                  float32\n",
      "shop_price_std                   float32\n",
      "shop_id_mean_encoded             float32\n",
      "shop_id_max_encoded              float32\n",
      "item_id_mean_encoded             float32\n",
      "item_id_max_encoded              float32\n",
      "city_mean_encoded                float32\n",
      "city_max_encoded                 float32\n",
      "item_category_id_mean_encoded    float32\n",
      "item_category_id_max_encoded     float32\n",
      "monthly_sales_item_lag_1         float32\n",
      "monthly_sales_shop_lag_1         float32\n",
      "item_price_mean_lag_1            float32\n",
      "monthly_sales_lag_1              float32\n",
      "monthly_sales_item_lag_2         float32\n",
      "monthly_sales_shop_lag_2         float32\n",
      "item_price_mean_lag_2            float32\n",
      "monthly_sales_lag_2              float32\n",
      "monthly_sales_item_lag_3         float32\n",
      "monthly_sales_shop_lag_3         float32\n",
      "item_price_mean_lag_3            float32\n",
      "monthly_sales_lag_3              float32\n",
      "monthly_sales_item_lag_6         float32\n",
      "monthly_sales_shop_lag_6         float32\n",
      "item_price_mean_lag_6            float32\n",
      "monthly_sales_lag_6              float32\n",
      "monthly_sales_item_lag_12        float32\n",
      "monthly_sales_shop_lag_12        float32\n",
      "item_price_mean_lag_12           float32\n",
      "monthly_sales_lag_12             float32\n",
      "monthly_sales_item_diff_12       float32\n",
      "monthly_sales_item_diff_23       float32\n",
      "monthly_sales_item_diff_36       float32\n",
      "monthly_sales_shop_diff_12       float32\n",
      "monthly_sales_shop_diff_23       float32\n",
      "monthly_sales_shop_diff_36       float32\n",
      "item_price_mean_diff_12          float32\n",
      "item_price_mean_diff_23          float32\n",
      "item_price_mean_diff_36          float32\n",
      "monthly_sales_diff_12            float32\n",
      "monthly_sales_diff_23            float32\n",
      "monthly_sales_diff_36            float32\n",
      "dtypes: float32(50), int32(4)\n",
      "memory usage: 2.3 GB\n"
     ]
    }
   ],
   "source": [
    "alldat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet,SGDRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat.drop(['monthly_sales_item',\n",
    "       'monthly_sales_shop'],axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = alldat.loc[:train_l-1].copy()\n",
    "test = alldat.loc[train_l:].copy()\n",
    "\n",
    "x_test = test.drop('monthly_sales',axis=1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into train and validation\n",
    "# the item-related  mean encoding of the data in validation set is assumed to null and filled with 0.3343\n",
    "# t1: latest period in training data, t2: validation period\n",
    "def train_val_split(t1,t2):\n",
    "    train = train_data.loc[train_data.date_block_num <= t1]\n",
    "    val = train_data.loc[train_data.date_block_num == t2]\n",
    "    \n",
    "    train_x = train.drop('monthly_sales',axis=1)\n",
    "    train_y = train['monthly_sales']\n",
    "    \n",
    "    val_x = val.drop('monthly_sales',axis=1)\n",
    "    val_y = val['monthly_sales']\n",
    "    \n",
    "    #adjust val set\n",
    "    #newp = newproduct(t2)\n",
    "    #val_x.loc[val_x.item_id.isin(newp),['item_id_mean_encoded','item_id_max_encoded','item_id-shop_id_mean_encoded',\n",
    "    #                                    'item_id-shop_id_max_encoded','month-item_id_mean_encoded','month-item_id_max_encoded']]=0.3343\n",
    "    \n",
    "    #train_x.drop(['city','item_id','shop_id','item_category_id'],axis=1,inplace=True)\n",
    "    #val_x.drop(['city','item_id','shop_id','item_category_id'],axis=1,inplace=True)\n",
    "\n",
    "    del train\n",
    "    del val\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y.clip(0,20)\n",
    "#'month-item_id_mean_encoded','item_id-city_mean_encoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeliing\n",
    "# 1. SGD(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3-fold cross validation\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler,MinMaxScaler\n",
    "sgddata_arr=[]\n",
    "for t in [33,28,23]:\n",
    "    x, y, x_val, y_val = train_val_split(t-1,t)\n",
    "    \n",
    "    mm = MinMaxScaler()\n",
    "    \n",
    "    x = mm.fit_transform(x)\n",
    "    x_val = mm.fit_transform(x_val)\n",
    "    y = np.log1p(y)\n",
    "    y_val = y_val.clip(0,20)\n",
    "    \n",
    "    data={}\n",
    "    data['x']=x \n",
    "    data['x_val'] = x_val\n",
    "    data['y'] = y\n",
    "    data['y_val'] = y_val\n",
    "    \n",
    "    sgddata_arr.append(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "# unoptimzied model\n",
    "for i in [0,1,2]:\n",
    "    sgd = SGDRegressor(l1_ratio=0.2, alpha=0.0005, penalty='elasticnet')\n",
    "    sgd.fit(sgddata_arr[i]['x'], sgddata_arr[i]['y'])\n",
    "        \n",
    "    y_pred =sgd.predict(sgddata_arr[i]['x_val'])\n",
    "    rmse = np.sqrt(mean_squared_error(np.expm1(y_pred).clip(0,20),sgddata_arr[i]['y_val']))\n",
    "    print('rmse in fold {}: {}'.format(i+1,rmse))\n",
    "    scores.append(rmse)\n",
    "     \n",
    "print('cv error is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparmeter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search space\n",
    "\n",
    "sgd_space={'l1_ratio':hp.uniform('l1_ratio',0.1,0.9),\n",
    "           'alpha':hp.uniform('alpha',0.00001,0.5)}\n",
    "#define the ojective function to be minimized\n",
    "#objective chosen to be cv errors\n",
    "def sgd_objective(params):\n",
    "    p = {'l1_ratio': params['l1_ratio'],\n",
    "        'alpha': params['alpha'],\n",
    "        'penalty':'elasticnet'\n",
    "       \n",
    "        }\n",
    "    score_arr=[]\n",
    "    print('start..............')\n",
    "    for i in [0,1,2]:\n",
    "        sgd = SGDRegressor(**p)\n",
    "        sgd.fit(sgddata_arr[i]['x'], sgddata_arr[i]['y'])\n",
    "        \n",
    "        y_pred =sgd.predict(sgddata_arr[i]['x_val'])\n",
    "        rmse = np.sqrt(mean_squared_error(np.expm1(y_pred).clip(0,20),sgddata_arr[i]['y_val']))\n",
    "        print('rmse in fold {}: {}'.format(i+1,rmse))\n",
    "        score_arr.append(rmse)\n",
    "     \n",
    "    print('cv error is {}'.format(np.mean(score_arr)))\n",
    "    print('-------------------------------------------------------\\n')\n",
    "    gc.collect() \n",
    "    return np.mean(score_arr)\n",
    "# tpe suggest : algorithm for find the best hyperparaters, max_evals =4\n",
    "sgd_best = fmin(fn = sgd_objective, space=sgd_space, algo=tpe.suggest, max_evals=15)\n",
    "print('best sgd elasticnet parameters is {}'.format(sgd_best)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unoptimzied model\n",
    "best_xgb = {'colsample_bytree': 1, 'gamma': 1.333080152263989, 'lambda': 3.4345783000200765, 'learning_rate': 0.25699972492763873, 'max_depth': 10, 'min_child_weight': 130.99821047754926, \n",
    "  'booster':'gbtree',\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'rmse','n_jobs':16,'subsample': 0.9}\n",
    "default =  {'booster': 'gbtree',\n",
    "          'eta': .1,\n",
    "          'min_child_weight': 100,\n",
    "          'max_depth': 10,\n",
    "          'objective': 'reg:linear',\n",
    "          'eval_metric': 'rmse',\n",
    "          'silent': False,\n",
    "          'nthread': 16}\n",
    "\n",
    "pred_arr=[]\n",
    "p={}\n",
    "t=33\n",
    "\n",
    "x,y,x_val,y_val = train_val_split(t-1,t)\n",
    "train = xgb.DMatrix(x,y)\n",
    "val = xgb.DMatrix(x_val,y_val)\n",
    "    \n",
    "model = xgb.train(default,train, 40, [(train, 'Train'), (val, 'Val')], early_stopping_rounds=5, verbose_eval=1 )\n",
    "y_pred = model.predict(val)\n",
    "pred_arr.append(np.sqrt(mean_squared_error(y_pred.clip(0,20),y_val)))\n",
    "    \n",
    "score = np.mean(pred_arr)\n",
    "print('validation error is {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(booster=model,max_num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "space = {'max_depth':hp.quniform('max_depth',6,12,1),\n",
    "        'learning_rate':hp.uniform('learning_rate',0.009,0.25),\n",
    "        'gamma':hp.uniform('gamma',0,5),\n",
    "        'min_child_weight':hp.uniform('min_child_weight',5,200),\n",
    "        'subsample':hp.uniform('subsample',0.4,1),\n",
    "        'colsample_bytree':hp.uniform('colsample_bytree',0.4,1),\n",
    "             'lambda':hp.uniform('lambda',0,5)\n",
    "            }\n",
    "\n",
    "def objective(params):\n",
    "    p ={'max_depth':int(params['max_depth']),\n",
    "    'learning_rate':params['learning_rate'],\n",
    "    'gamma': params['gamma'],\n",
    "    'min_child_weight': params['min_child_weight'],\n",
    "    'subsample': params['subsample'],\n",
    "    'colsample_bytree': params['colsample_bytree'],\n",
    "    'n_jobs':16,\n",
    "    'lambda':params['lambda'],\n",
    "    'booster':'gbtree',\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'rmse'}\n",
    "\n",
    "    pred_arr=[]\n",
    "    for t in [33,28,23]:\n",
    "        x,y,x_val,y_val = train_val_split(t-1,t)\n",
    "        train = xgb.DMatrix(x,y)\n",
    "        val = xgb.DMatrix(x_val,y_val)\n",
    "    \n",
    "        model = xgb.train(p, train, 25, [(train, 'Train'), (val, 'Val')], early_stopping_rounds=5, verbose_eval=25 )\n",
    "        y_pred = model.predict(val)\n",
    "        pred_arr.append(np.sqrt(mean_squared_error(y_pred.clip(0,20),y_val)))\n",
    "\n",
    "        \n",
    "\n",
    "    score = np.mean(pred_arr)\n",
    "    print('cross validation error is {}'.format(score))\n",
    "    print('parameter: {}'.format(p))\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(fn = objective , space = space , algo=tpe.suggest, max_evals=8)\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_xgb = {'colsample_bytree': 0.7593842604193011, 'gamma': 0.7662167628900601, 'lambda': 1.083692999543464, 'learning_rate': 0.3368036151298407, 'max_depth': 10, 'min_child_weight': 121.58843063826328, 'subsample': 0.6133697129110832, \n",
    " 'booster':'gbtree',\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'rmse','n_jobs':16}\n",
    "#xgb{'colsample_bytree': 0.580713150681273, 'gamma': 3.281909006326122, 'lambda': 0.9862477340531345, 'learning_rate': 0.4613177050489606, 'max_depth': 9.0, 'min_child_weight': 143.88597768322091, 'subsample': 0.6787939288312543}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_xgb = {'colsample_bytree': 0.830688370955194, 'gamma': 1.333080152263989, 'lambda': 3.4345783000200765, 'learning_rate': 0.22699972492763873, 'max_depth': 8, 'min_child_weight': 130.99821047754926, \n",
    "  'booster':'gbtree',\n",
    "    'objective':'reg:linear',\n",
    "    'eval_metric':'rmse','n_jobs':16,'subsample': 0.4960204984048794}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgb=[]\n",
    "de_p={ 'metric': 'l2','objective':'regression'}\n",
    "for t in [33,28,23]:\n",
    "    x,y,x_val,y_val = train_val_split(t-1,t)\n",
    "    train_lgb = lightgbm.Dataset(x,y)\n",
    "    val_lgb = lightgbm.Dataset(x_val,y_val)\n",
    "    model = lightgbm.train(de_p,train_lgb,150,valid_sets=[train_lgb,val_lgb],early_stopping_rounds=100,verbose_eval=50)\n",
    "    \n",
    "    pred_y = model.predict(x_val, num_iteration=model.best_iteration or 150)\n",
    "    \n",
    "    score = np.sqrt(mean_squared_error(pred_y.clip(0,20),y_val))\n",
    "    pred_lgb.append(score)\n",
    "print('cv error: {}'.format(np.mean(pred_lgb))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lgb_params = {\n",
    "     'num_leaves': 200,\n",
    "    'objective': 'regression',\n",
    "    'min_data_in_leaf': 100,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'num_threads': 16\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "space = {'num_leaves':hp.quniform('num_leaves',50,1000,50),\n",
    "    'learning_rate':hp.uniform('learning_rate',0.01,0.5),\n",
    "    'min_data_in_leaf':hp.quniform('min_data_in_leaf',50,800,50),\n",
    "    \n",
    "    'feature_fraction':hp.uniform('feature_fraction',0.4,1),\n",
    "    'bagging_fraction':hp.uniform('bagging_fraction',0.4,1),\n",
    "             'bagging_freq':hp.quniform('bagging_freq',1,5,1)\n",
    "    }\n",
    "\n",
    "def objective(params):\n",
    "    p ={'num_leaves':int(params['num_leaves']),\n",
    "    'learning_rate':params['learning_rate'],\n",
    "    'min_data_in_leaf': int(params['min_data_in_leaf']),\n",
    "\n",
    "    'feature_fraction': params['feature_fraction'],\n",
    "    'bagging_fraction': params['bagging_fraction'],\n",
    "    'num_threads':16,\n",
    "    'bagging_freq':int(params['bagging_freq']),\n",
    "        \n",
    "    'objective':'regression',\n",
    "    'metric':'l2'}\n",
    "    pred_lgb = []\n",
    "    for t in [33,28,23]:\n",
    "        x,y,x_val,y_val = train_val_split(t-1,t)\n",
    "        train_lgb = lightgbm.Dataset(x,y)\n",
    "        val_lgb = lightgbm.Dataset(x_val,y_val)\n",
    "        model = lightgbm.train(p, train_lgb,1500,valid_sets=[train_lgb,val_lgb],early_stopping_rounds=100,verbose_eval=250)\n",
    "    \n",
    "        pred_y = model.predict(x_val, num_iteration=model.best_iteration or 2500)\n",
    "    \n",
    "        score = np.sqrt(mean_squared_error(pred_y.clip(0,20),y_val))\n",
    "        pred_lgb.append(score)\n",
    "        \n",
    "        \n",
    "    print('parameters: {}'.format(p))    \n",
    "    print('cv error: {}'.format(np.mean(pred_lgb)))    \n",
    "    return np.mean(pred_lgb)\n",
    "\n",
    "best_lgb = fmin(fn = objective , space = space , algo=tpe.suggest, max_evals=10)\n",
    "\n",
    "print('best parameters for lgb is: {}'.format(best_lgb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgb = {'bagging_fraction': 0.8870127757104167, 'bagging_freq': 5, 'feature_fraction': 0.8554121488511988, 'learning_rate': 0.03365956768833352, 'min_data_in_leaf': 450, \n",
    "'num_leaves': 150, 'num_threads':16,'objective':'regression','metric':'l2'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = alldat.loc[alldat.date_block_num<33]\n",
    "val_x = alldat.loc[alldat.date_block_num==33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "train_list=[]\n",
    "val_list=[]\n",
    "for f in ['shop_id','item_id','date_block_num','city','item_category_id']:\n",
    "    unique_val = train_data[f].unique()\n",
    "    label_map={}\n",
    "    print(len(unique_val))\n",
    "    for val in range(len(unique_val)):\n",
    "        label_map[unique_val[val]] = val\n",
    "        \n",
    "    train_list.append(train_x[f].map(label_map).values)\n",
    "    val_list.append(val_x[f].map(label_map).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_x['monthly_sales']\n",
    "y_val = val_x['monthly_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.drop('monthly_sales',axis=1, inplace=True)\n",
    "val_x.drop('monthly_sales',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,Reshape, Concatenate,Input,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "inputs=[]\n",
    "embeddings=[]\n",
    "# embedding layers  \n",
    "shop = Input(shape=(1,))\n",
    "embedding = Embedding(60,15,input_length=1)(shop)\n",
    "embedding = Reshape(target_shape=(15,))(embedding)\n",
    "inputs.append(shop)\n",
    "embeddings.append(embedding)\n",
    "    \n",
    "item = Input(shape=(1,))\n",
    "embedding = Embedding(21807,200,input_length=1)(item)\n",
    "embedding = Reshape(target_shape=(200,))(embedding)\n",
    "inputs.append(item)\n",
    "embeddings.append(embedding)\n",
    "    \n",
    "date = Input(shape=(1,))\n",
    "embedding = Embedding(34,15,input_length=1)(date)\n",
    "embedding = Reshape(target_shape=(15,))(embedding)\n",
    "inputs.append(date)\n",
    "embeddings.append(embedding)\n",
    "    \n",
    "city = Input(shape=(1,))\n",
    "embedding = Embedding(32,10,input_length=1)(city)\n",
    "embedding = Reshape(target_shape=(10,))(embedding)\n",
    "inputs.append(city)\n",
    "embeddings.append(embedding)\n",
    "    \n",
    "category = Input(shape=(1,))\n",
    "embedding = Embedding(84,20,input_length=1)(category)\n",
    "embedding = Reshape(target_shape=(20,))(embedding)\n",
    "inputs.append(category)\n",
    "embeddings.append(embedding)\n",
    "    \n",
    "numeric = Input(shape=(98,))\n",
    "embedding = Dense(30)(numeric)\n",
    "inputs.append(numeric)\n",
    "embeddings.append(embedding)\n",
    "\n",
    "x = Concatenate()(embeddings)\n",
    "x = Dense(350, kernel_initializer='uniform')(x)\n",
    "# parametrized relu\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "    \n",
    "x = Dense(50, kernel_initializer='uniform')(x)\n",
    "x = PReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "    \n",
    "output = Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "model = Model(inputs, output)\n",
    "    \n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max sclaing the numerical features\n",
    "normalized_f = alldat.columns.difference(['item_category_id','city','item_id','shop_id','date_block_num','monthly_sales']) \n",
    "\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(train_data[normalized_f])\n",
    "\n",
    "train_nn = mm.transform(train_x[normalized_f])\n",
    "val_nn = mm.transform(val_x[normalized_f])\n",
    "\n",
    "train_list.append(train_nn)\n",
    "val_list.append(val_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "np.random.seed(7)\n",
    "val_rmse={}\n",
    "n=0\n",
    "count=0\n",
    "checkpoint = ModelCheckpoint('nn_weights.hdf5', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kerasModel = KerasRegressor(build_fn=baseline_model, epochs=5, batch_size=2048,verbose=1)\n",
    "#stop training after validation error increase for a specific number of rounds\n",
    "while(True):\n",
    "    print('round {}'.format(count+1))\n",
    "    model.fit(train_list,np.log1p(y),epochs=1, batch_size=2048,verbose=1,callbacks=[checkpoint])\n",
    "    y_pred = model.predict(val_list)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val.clip(0,20), np.expm1(y_pred).clip(0,20)))\n",
    "    print(rmse)\n",
    "    \n",
    "    val_rmse[count] =rmse\n",
    "    if(count>0):\n",
    "        if(val_rmse[count]>val_rmse[count-1]):\n",
    "            n=n+1\n",
    "    count= count+1\n",
    "    if(n>=3):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_list,np.log1p(y),epochs=3, batch_size=2048,verbose=1)\n",
    "y_pred = model.predict(val_list)\n",
    "rmse = np.sqrt(mean_squared_error(y_val.clip(0,20), np.round(np.expm1(y_pred)).clip(0,20)))\n",
    "print('rmse of optimized NN model is {}'.format(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb=[]\n",
    "pred_lgb=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating meta features for xgb\n",
    "for t in [29,30,31,32]:\n",
    "    x,y,x_val,y_val  = train_val_split(t-1,t)\n",
    "    train = xgb.DMatrix(x,y)\n",
    "    val = xgb.DMatrix(x_val,y_val)\n",
    "    \n",
    "    model = xgb.train(best_xgb, train, 60, [(train, 'Train'), (val, 'Val')], early_stopping_rounds=20, verbose_eval=10 )\n",
    "    y_pred = model.predict(val)\n",
    "    \n",
    "    pred_xgb.append(y_pred.clip(0,20))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb = np.concatenate(pred_xgb,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating meta features for xgb\n",
    "for t in [29,30,31,32]:\n",
    "    x,y,x_val,y_val  = train_val_split(t-1,t)\n",
    "    train_lgb = lightgbm.Dataset(x,y)\n",
    "    val_lgb = lightgbm.Dataset(x_val,y_val)\n",
    "    model = lightgbm.train(best_lgb, train_lgb,2500,valid_sets=[train_lgb,val_lgb],early_stopping_rounds=500,verbose_eval=250)\n",
    "    \n",
    "    pred_y = model.predict(x_val, num_iteration=model.best_iteration or 2500)\n",
    "    \n",
    "    pred_lgb.append(pred_y.clip(0,20))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgb = np.concatenate(pred_lgb,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_level2 = train_data.loc[train_data.date_block_num.isin([29,30,31,32]),'monthly_sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.linspace(0,1,10000)\n",
    "rmse = 100\n",
    "best_weight=0 \n",
    "for i in weight:\n",
    "    pred = i * pred_xgb + (1-i) * pred_lgb\n",
    "    score = np.sqrt(mean_squared_error(pred, y_train_level2.clip(0,20)))\n",
    "    if(score<rmse):\n",
    "        rmse = score\n",
    "        best_weight=i\n",
    "print('the best score is {}'.format(rmse))\n",
    "print('the best weight is {}'.format(best_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'a':pred_xgb}).to_pickle('./pred_xgb.pkl')\n",
    "pd.DataFrame({'a':pred_lgb}).to_pickle('./pred_lgb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,x_val,y_val  = train_val_split(32,33)\n",
    "train = xgb.DMatrix(x,y)\n",
    "val = xgb.DMatrix(x_val,y_val)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.train(best_xgb, train, 60, [(train, 'Train'), (val, 'Val')], early_stopping_rounds=20, verbose_eval=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_xgb33 = model_xgb.predict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgb = {'bagging_fraction': 0.8870127757104167, 'bagging_freq': 5, 'feature_fraction': 0.8554121488511988, 'learning_rate': 0.03365956768833352, 'min_data_in_leaf': 450, \n",
    "'num_leaves': 150, 'num_threads':16,'objective':'regression','metric':'l2'}\n",
    "basic = {'num_threads':16,'objective':'regression','metric':'l2'}\n",
    "x,y,x_val,y_val  = train_val_split(32,33)\n",
    "train_lgb = lightgbm.Dataset(x,y)\n",
    "val_lgb = lightgbm.Dataset(x_val,y_val)\n",
    "model = lightgbm.train(basic, train_lgb,3000,valid_sets=[train_lgb,val_lgb],early_stopping_rounds=500,verbose_eval=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lgb33 = model.predict(x_val, num_iteration=model.best_iteration or 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.sqrt(mean_squared_error(pred_lgb33.clip(0,20),y_val))\n",
    "print('after ensemble, the rmse is {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_lgb = model.predict(x_test[x.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_xgb = model_xgb.predict(xgb.DMatrix(x_test[x.columns]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = test_pred_lgb.clip(0,20)*(1-best_weight) + test_pred_xgb.clip(0,20)*(best_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit=pd.DataFrame({'ID':id_test,'item_cnt_month':test_pred_lgb }).set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
